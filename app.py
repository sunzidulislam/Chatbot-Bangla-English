import streamlit as st
import pandas as pd
import base64
import sys
from datetime import datetime
import fitz  # PyMuPDF
import re
import numpy as np

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate

sys.stdout.reconfigure(encoding='utf-8')

def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        with fitz.open(stream=pdf.read(), filetype="pdf") as doc:
            for page in doc:
                page_text = page.get_text("text")
                if page_text:
                    cleaned = page_text.replace('\xa0', ' ').strip()
                    text += cleaned + "\n"
    return text

def preprocess_text(text):
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

def get_text_chunks(text, model_name):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500 if model_name == "Google AI" else 1000,
        chunk_overlap=400 if model_name == "Google AI" else 200,
        separators=["\n\n", "\n", ".", "‡•§", "!", "?"]
    )
    return splitter.split_text(text)

def get_vector_store(text_chunks, model_name, api_key):
    if model_name != "Google AI":
        raise ValueError("Unsupported model")
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", google_api_key=api_key
    )
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")
    return vector_store

def get_conversational_chain(model_name, vectorstore, api_key):
    if model_name != "Google AI":
        raise ValueError("Unsupported model")

    prompt_template = """
    ‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶æ‡¶® ‡¶ì ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶∏‡ßç‡¶§ ‡¶∏‡¶π‡¶ï‡¶æ‡¶∞‡ßÄ‡•§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü ‡¶¨‡¶æ ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø‡¶§‡ßá ‡¶¶‡¶ø‡¶®‡•§
    ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶®‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶¨‡¶≤‡ßÅ‡¶®: "‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶®‡ßá‡¶á"‡•§ ‡¶≠‡ßÅ‡¶≤ ‡¶§‡¶•‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§

    ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø:
    {context}

    ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®:
    {question}

    ‡¶â‡¶§‡ßç‡¶§‡¶∞:
    """
    model = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro", temperature=0.3, google_api_key=api_key
    )
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    return load_qa_chain(model, chain_type="stuff", prompt=prompt)

# New cosine similarity using embeddings
def cosine_similarity_score_embedding(query, context_chunks, embeddings_model):
    # Embed query
    query_emb = embeddings_model.embed_query(query)
    # Embed contexts
    context_embs = embeddings_model.embed_documents(context_chunks)

    # Normalize vectors
    def normalize(vec):
        norm = np.linalg.norm(vec)
        return vec / norm if norm > 0 else vec

    query_emb = normalize(query_emb)
    context_embs = [normalize(emb) for emb in context_embs]

    sims = [np.dot(query_emb, emb) for emb in context_embs]

    return float(np.mean(sims))

def groundedness_score(answer, retrieved_chunks):
    def tokenize(text):
        return re.findall(r'\b\w+\b', text.lower())

    answer_tokens = set(tokenize(answer))
    context_tokens = set(tokenize(" ".join(retrieved_chunks)))

    if not answer_tokens:
        return 0.0
    return round(len(answer_tokens & context_tokens) / len(answer_tokens), 2)

def chat_display(question, answer):
    return f"""
    <style>
        .chat-message {{ padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex; }}
        .chat-message.user {{ background-color: #2b313e; }}
        .chat-message.bot {{ background-color: #475063; }}
        .chat-message .avatar img {{ max-width: 78px; border-radius: 50%; object-fit: cover; }}
        .chat-message .message {{ width: 80%; padding: 0 1.5rem; color: #fff; white-space: pre-wrap; }}
    </style>
    <div class="chat-message user">
        <div class="avatar"><img src="https://i.ibb.co/CKpTnWr/user-icon-2048x2048-ihoxz4vq.png" alt="User"/></div>
        <div class="message">{question}</div>
    </div>
    <div class="chat-message bot">
        <div class="avatar"><img src="https://i.ibb.co/wNmYHsx/langchain-logo.webp" alt="Bot"/></div>
        <div class="message">{answer}</div>
    </div>
    """

def user_input(user_question, model_name, api_key, pdf_docs, conversation_history):
    if not api_key or not pdf_docs:
        st.warning("PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç API key ‡¶¶‡¶ø‡¶®‡•§")
        return

    raw_text = get_pdf_text(pdf_docs)
    raw_text = preprocess_text(raw_text)
    text_chunks = get_text_chunks(raw_text, model_name)
   # st.sidebar.write(f"üî¢ ‡¶Æ‡ßã‡¶ü ‡¶ö‡¶æ‡¶Ç‡¶ï ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ: {len(text_chunks)}")

    # Create vector store and save locally
    vector_store = get_vector_store(text_chunks, model_name, api_key)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

    # Retrieve top 10 relevant docs
    docs = new_db.similarity_search(user_question, k=10)

    # for i, doc in enumerate(docs):
       # st.sidebar.text_area(f"‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶Ö‡¶Ç‡¶∂ {i+1}", doc.page_content[:1500], height=150)

    chain = get_conversational_chain(model_name, vectorstore=new_db, api_key=api_key)
    response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)

    user_question_output = user_question
    response_output = response['output_text']

    retrieved_texts = [doc.page_content for doc in docs]

    # Use embedding-based cosine similarity score
    cosine_score = round(cosine_similarity_score_embedding(user_question, retrieved_texts, embeddings), 2)
    grounded_score = groundedness_score(response_output, retrieved_texts)

    pdf_names = [pdf.name for pdf in pdf_docs]
    conversation_history.append((
        user_question_output,
        response_output,
        model_name,
        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        ", ".join(pdf_names),
        cosine_score,
        grounded_score
    ))

    st.markdown(chat_display(user_question_output, response_output), unsafe_allow_html=True)
    st.success(f"üìè Cosine Similarity: **{cosine_score}** | üìö Groundedness: **{grounded_score}**")

    for question, answer, _, _, _, cos, ground in reversed(conversation_history[:-1]):
        st.markdown(chat_display(question, answer), unsafe_allow_html=True)
        st.info(f"üìè Cosine Similarity: **{cos}** | üìö Groundedness: **{ground}**")

    if conversation_history:
        df = pd.DataFrame(conversation_history, columns=[
            "Question", "Answer", "Model", "Timestamp", "PDF Name", "Cosine Similarity", "Groundedness Score"
        ])
        csv = df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="conversation_history.csv"><button>Download conversation history as CSV</button></a>'
        st.sidebar.markdown(href, unsafe_allow_html=True)

def main():
    st.set_page_config(page_title="‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ PDF ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßã‡¶§‡ßç‡¶§‡¶∞", page_icon="üìö")
    st.header("‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ PDF ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßã‡¶§‡ßç‡¶§‡¶∞ ‡¶¨‡¶ü üìö")

    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []

    model_name = st.sidebar.radio("‡¶Æ‡¶°‡ßá‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®:", ("Google AI",))

    api_key = None
    if model_name == "Google AI":
        api_key = st.sidebar.text_input("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Google API Key ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:", type="password")
        st.sidebar.markdown("API key ‡¶™‡ßá‡¶§‡ßá [‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ø‡¶æ‡¶®](https://ai.google.dev/)‡•§")
        if not api_key:
            st.sidebar.warning("‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Google API Key ‡¶¶‡¶ø‡¶®‡•§")
            return

    with st.sidebar:
        st.title("‡¶®‡¶ø‡¶Ø‡¶º‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶£ ‡¶Æ‡ßá‡¶®‡ßÅ:")
        col1, col2 = st.columns(2)
        reset_button = col2.button("‡¶∞‡¶ø‡¶∏‡ßá‡¶ü")
        clear_button = col1.button("‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡ßü ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®")

        if reset_button:
            st.session_state.conversation_history = []
            st.session_state.user_question = None
        elif clear_button:
            if 'user_question' in st.session_state:
                st.warning("‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶¨‡¶æ‡¶§‡¶ø‡¶≤ ‡¶π‡¶¨‡ßá‡•§")
                st.session_state.user_question = ""
                if st.session_state.conversation_history:
                    st.session_state.conversation_history.pop()

        pdf_docs = st.file_uploader("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®", accept_multiple_files=True)
        if st.button("‡¶∏‡¶æ‡¶¨‡¶Æ‡¶ø‡¶ü ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®"):
            if pdf_docs:
                with st.spinner("‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶π‡¶ö‡ßç‡¶õ‡ßá..."):
                    st.success("‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶π‡ßü‡ßá‡¶õ‡ßá ‚úÖ")
            else:
                st.warning("‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")

    user_question = st.text_input("PDF ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü/English)")
    if user_question:
        user_input(user_question, model_name, api_key, pdf_docs, st.session_state.conversation_history)
        st.session_state.user_question = ""

if __name__ == "__main__":
    main()
