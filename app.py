import streamlit as st
import pandas as pd
import base64
import sys
from datetime import datetime

import fitz  # pip install pymupdf
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate

# Ensure stdout utf-8 encoding for Bangla
sys.stdout.reconfigure(encoding='utf-8')

def get_pdf_text(pdf_docs):
    """Extract text from PDF docs using PyMuPDF (fitz)."""
    text = ""
    for pdf in pdf_docs:
        with fitz.open(stream=pdf.read(), filetype="pdf") as doc:
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text("text")
                if page_text:
                    cleaned = page_text.replace('\xa0', ' ').strip()
                    text += cleaned + "\n"
    return text

def preprocess_text(text):
    """Basic cleaning to normalize text for better chunking."""
    # Remove excessive spaces and normalize newlines
    import re
    text = re.sub(r'\n{3,}', '\n\n', text)  # max two newlines
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

def get_text_chunks(text, model_name):
    """Split text into chunks for vector store."""
    if model_name == "Google AI":
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,
            chunk_overlap=400,
            separators=["\n\n", "\n", ".", "‡•§", "!", "?"]
        )
    else:
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_text(text)
    return chunks

def get_vector_store(text_chunks, model_name, api_key=None):
    if model_name == "Google AI":
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    else:
        raise ValueError("Unsupported model")
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_index")
    return vector_store

def get_conversational_chain(model_name, vectorstore=None, api_key=None):
    if model_name == "Google AI":
        prompt_template = """
        ‡¶Ü‡¶™‡¶®‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶æ‡¶® ‡¶ì ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡¶∏‡ßç‡¶§ ‡¶∏‡¶π‡¶ï‡¶æ‡¶∞‡ßÄ‡•§ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶∏‡¶†‡¶ø‡¶ï ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü ‡¶¨‡¶æ ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø‡¶§‡ßá ‡¶¶‡¶ø‡¶®‡•§
        ‡¶Ø‡¶¶‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶®‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶¨‡¶≤‡ßÅ‡¶®: "‡¶è‡¶á ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶®‡ßá‡¶á"‡•§ ‡¶≠‡ßÅ‡¶≤ ‡¶§‡¶•‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡ßá‡¶® ‡¶®‡¶æ‡•§

        ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶§‡¶•‡ßç‡¶Ø:
        {context}

        ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®:
        {question}

        ‡¶â‡¶§‡ßç‡¶§‡¶∞:
        """
        model = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.3, google_api_key=api_key)
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
        return chain
    else:
        raise ValueError("Unsupported model")

def user_input(user_question, model_name, api_key, pdf_docs, conversation_history):
    if api_key is None or pdf_docs is None:
        st.warning("PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç API key ‡¶¶‡¶ø‡¶®‡•§")
        return
    
    raw_text = get_pdf_text(pdf_docs)
    raw_text = preprocess_text(raw_text)
    text_chunks = get_text_chunks(raw_text, model_name)
    
    # For debug: Show number of chunks
    st.sidebar.write(f"üî¢ ‡¶Æ‡ßã‡¶ü ‡¶ö‡¶æ‡¶Ç‡¶ï ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ: {len(text_chunks)}")
    
    vector_store = get_vector_store(text_chunks, model_name, api_key)

    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    docs = new_db.similarity_search(user_question, k=10)

    # Debug: Show matched chunks for user
    for i, doc in enumerate(docs):
        st.sidebar.text_area(f"‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶Ö‡¶Ç‡¶∂ {i+1}", doc.page_content[:1500], height=150)

    chain = get_conversational_chain(model_name, vectorstore=new_db, api_key=api_key)
    response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
    
    user_question_output = user_question
    response_output = response['output_text']

    pdf_names = [pdf.name for pdf in pdf_docs] if pdf_docs else []
    conversation_history.append((user_question_output, response_output, model_name, datetime.now().strftime('%Y-%m-%d %H:%M:%S'), ", ".join(pdf_names)))

    # Display latest QA
    st.markdown(
        f"""
        <style>
            .chat-message {{
                padding: 1.5rem;
                border-radius: 0.5rem;
                margin-bottom: 1rem;
                display: flex;
            }}
            .chat-message.user {{
                background-color: #2b313e;
            }}
            .chat-message.bot {{
                background-color: #475063;
            }}
            .chat-message .avatar {{
                width: 20%;
            }}
            .chat-message .avatar img {{
                max-width: 78px;
                max-height: 78px;
                border-radius: 50%;
                object-fit: cover;
            }}
            .chat-message .message {{
                width: 80%;
                padding: 0 1.5rem;
                color: #fff;
                white-space: pre-wrap;
            }}
        </style>
        <div class="chat-message user">
            <div class="avatar">
                <img src="https://i.ibb.co/CKpTnWr/user-icon-2048x2048-ihoxz4vq.png" alt="User"/>
            </div>    
            <div class="message">{user_question_output}</div>
        </div>
        <div class="chat-message bot">
            <div class="avatar">
                <img src="https://i.ibb.co/wNmYHsx/langchain-logo.webp" alt="Bot" />
            </div>
            <div class="message">{response_output}</div>
        </div>
        """,
        unsafe_allow_html=True
    )

    # Show previous conversation history
    for question, answer, m_name, timestamp, pdf_name in reversed(conversation_history[:-1]):
        st.markdown(
            f"""
            <div class="chat-message user">
                <div class="avatar">
                    <img src="https://i.ibb.co/CKpTnWr/user-icon-2048x2048-ihoxz4vq.png" alt="User"/>
                </div>    
                <div class="message">{question}</div>
            </div>
            <div class="chat-message bot">
                <div class="avatar">
                    <img src="https://i.ibb.co/wNmYHsx/langchain-logo.webp" alt="Bot"/>
                </div>
                <div class="message">{answer}</div>
            </div>
            """,
            unsafe_allow_html=True
        )
    
    # CSV Download of history
    if len(conversation_history) > 0:
        df = pd.DataFrame(conversation_history, columns=["Question", "Answer", "Model", "Timestamp", "PDF Name"])
        csv = df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="conversation_history.csv"><button>Download conversation history as CSV file</button></a>'
        st.sidebar.markdown(href, unsafe_allow_html=True)
        st.markdown("CSV ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶æ‡¶Æ ‡¶™‡¶æ‡¶∂‡ßá ‡¶¨‡¶æ‡¶ü‡¶®‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")

    st.snow()

def main():
    st.set_page_config(page_title="‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ PDF ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßã‡¶§‡ßç‡¶§‡¶∞", page_icon="üìö")
    st.header("‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ PDF ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßã‡¶§‡ßç‡¶§‡¶∞ ‡¶¨‡¶ü üìö")

    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []

    model_name = st.sidebar.radio("‡¶Æ‡¶°‡ßá‡¶≤ ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®:", ("Google AI",))

    api_key = None
    if model_name == "Google AI":
        api_key = st.sidebar.text_input("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ Google API Key ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:", type="password")
        st.sidebar.markdown("API key ‡¶™‡ßá‡¶§‡ßá [‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ø‡¶æ‡¶®](https://ai.google.dev/)‡•§")
        if not api_key:
            st.sidebar.warning("‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø Google API Key ‡¶¶‡¶ø‡¶®‡•§")
            return

    with st.sidebar:
        st.title("‡¶®‡¶ø‡¶Ø‡¶º‡¶®‡ßç‡¶§‡ßç‡¶∞‡¶£ ‡¶Æ‡ßá‡¶®‡ßÅ:")
        col1, col2 = st.columns(2)
        reset_button = col2.button("‡¶∞‡¶ø‡¶∏‡ßá‡¶ü")
        clear_button = col1.button("‡¶™‡ßÅ‡¶®‡¶∞‡¶æ‡ßü ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®")

        if reset_button:
            st.session_state.conversation_history = []
            st.session_state.user_question = None
            api_key = None
            pdf_docs = None
        elif clear_button:
            if 'user_question' in st.session_state:
                st.warning("‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶¨‡¶æ‡¶§‡¶ø‡¶≤ ‡¶π‡¶¨‡ßá‡•§")
                st.session_state.user_question = ""
                if len(st.session_state.conversation_history) > 0:
                    st.session_state.conversation_history.pop()

        pdf_docs = st.file_uploader("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®", accept_multiple_files=True)
        if st.button("‡¶∏‡¶æ‡¶¨‡¶Æ‡¶ø‡¶ü ‡¶ì ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®"):
            if pdf_docs:
                with st.spinner("‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶π‡¶ö‡ßç‡¶õ‡ßá..."):
                    st.success("‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶π‡ßü‡ßá‡¶õ‡ßá ‚úÖ")
            else:
                st.warning("‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá PDF ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®‡•§")

    user_question = st.text_input("PDF ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡ßü/English)")

    if user_question:
        user_input(user_question, model_name, api_key, pdf_docs, st.session_state.conversation_history)
        st.session_state.user_question = ""

if __name__ == "__main__":
    main()
